<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on apcj@f5 blog</title>
    <link>/tags/aws/</link>
    <description>Recent content in aws on apcj@f5 blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes | Ephemeral Kubernetes Lab with IaC and GitOps</title>
      <link>/posts/ephemeral-kubernetes-lab-with-iac-and-gitops/</link>
      <pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/ephemeral-kubernetes-lab-with-iac-and-gitops/</guid>
      <description>I&amp;rsquo;ve been thinking of moving my Kubernetes lab into the cloud, but with cloud resource usage being scrutinized by the IT department, running them 24x7 the way I&amp;rsquo;m used to is a no-go. I need a setup that meets the following requirements:
 Simple to create and tear down Applications must be pre-deployed when the cluster is up, as close to &amp;ldquo;just the way I left it there last night&amp;rdquo; as possible cost $0 when the setup has been switched off  I eventually settled on the idea of an ephemeral Kubernetes lab environment using Infrastructure as Code (IaC) and GitOps practices, which I will cover in this post.</description>
      <content>&lt;p&gt;I&amp;rsquo;ve been thinking of moving my Kubernetes lab into the cloud, but with cloud resource usage being scrutinized by the IT department, running them 24x7 the way I&amp;rsquo;m used to is a no-go. I need a setup that meets the following requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple to create and tear down&lt;/li&gt;
&lt;li&gt;Applications must be pre-deployed when the cluster is up, as close to &amp;ldquo;just the way I left it there last night&amp;rdquo; as possible&lt;/li&gt;
&lt;li&gt;cost $0 when the setup has been switched off&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I eventually settled on the idea of an ephemeral Kubernetes lab environment using &lt;a href=&#34;#infrastructure-as-code-using-terraform&#34;&gt;Infrastructure as Code (IaC)&lt;/a&gt; and &lt;a href=&#34;#k8s-resource-management-via-gitops-using-argo-cd&#34;&gt;GitOps&lt;/a&gt; practices, which I will cover in this post. You can follow along by cross referencing the code found here:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/leonseng/terraform-everything/tree/master/eks-gitops&#34;&gt;https://github.com/leonseng/terraform-everything/tree/master/eks-gitops&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;infrastructure-as-code-using-terraform&#34;&gt;Infrastructure as Code using Terraform&lt;/h1&gt;
&lt;p&gt;Starting with the Kubernetes cluster, using a managed Kubernetes offering makes sense for me as my current focus is on Kubernetes applications.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If cluster customization is of importance, one can deploy Kubernetes on the cloud computes using tools like &lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;kops&lt;/a&gt; or &lt;a href=&#34;https://github.com/kubernetes-sigs/kubespray&#34;&gt;kubespray&lt;/a&gt;, similarly adopting IaC practices detailed below.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I went with &lt;a href=&#34;https://aws.amazon.com/eks/&#34;&gt;EKS&lt;/a&gt; as I am more familiar with AWS. An EKS cluster (or any other managed Kubernetes offerings in the public cloud) has a lot of dependencies on other cloud resources, such as computes, gateways, security policies and more. Rather than figuring out &lt;em&gt;when&lt;/em&gt; to create &lt;em&gt;what&lt;/em&gt;, &lt;a href=&#34;https://www.terraform.io/&#34;&gt;Terraform&lt;/a&gt; can be used to define all the cloud resources needed for a functional EKS cluster in a declarative manner. In addition, Terraform modules such as &lt;a href=&#34;https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest&#34;&gt;vpc&lt;/a&gt; and &lt;a href=&#34;https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest&#34;&gt;eks&lt;/a&gt; can be used to further abstract away the web of dependencies. All it takes is a couple of Terraform modules and some data sources to create the cluster, as seen here:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/leonseng/terraform-everything/blob/master/eks-gitops/eks.tf&#34;&gt;https://github.com/leonseng/terraform-everything/blob/master/eks-gitops/eks.tf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With the cluster sorted, I turn my attention to deploying Kubernetes resources/applications in the cluster.&lt;/p&gt;
&lt;h1 id=&#34;k8s-resource-management-via-gitops-using-argo-cd&#34;&gt;K8s resource management via GitOps using Argo CD&lt;/h1&gt;
&lt;p&gt;It&amp;rsquo;s instinctive to start running &lt;code&gt;kubectl&lt;/code&gt; commands to deploy Kubernetes resources that make up your application in the cluster, but keeping track of them gets harder overtime. If the cluster is to be rebuilt on a regular basis, we&amp;rsquo;d best hope we have a record of what&amp;rsquo;s deployed in it, and what better option than storing the Kubernetes manifests in a Git repository where the IaC definitions are also stored. With the desired state of applications in Git, the next step is ensuring the cluster state reflects the desired state. This is where GitOps comes in.&lt;/p&gt;
&lt;p&gt;GitOps can be briefly described as an automated workflow that ensures the state of the cluster and its applications matches the desired state from the source of truth, or Git repositories in the case. GitOps achieves this with one of the two patterns below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Changes are &lt;strong&gt;pushed&lt;/strong&gt; to the cluster&lt;/p&gt;
&lt;p&gt;Continuous deployment workflow applies the resource manifests on our cluster whenever there are changes in the Git repository. This can be done using simple Bash scripts, or orchestration tools like Ansible. A problem arises when there are multiple deployment workflows targeting the same cluster. As each workflow may not have a complete view of the cluster, it could lead to an inconsistent state in the cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Changes are &lt;strong&gt;pulled&lt;/strong&gt; into the cluster&lt;/p&gt;
&lt;p&gt;Officially the preferred pattern in &lt;a href=&#34;https://opengitops.dev/blog/1.0-announcement/&#34;&gt;OpenGitOps v1.0.0&lt;/a&gt; - an agent in the cluster continuously polls the desired state from a remote Git repository, and applies the changes in the cluster. &lt;a href=&#34;https://argo-cd.readthedocs.io/en/stable/%5D&#34;&gt;Argo CD&lt;/a&gt; and &lt;a href=&#34;https://fluxcd.io/docs/&#34;&gt;Flux&lt;/a&gt; are two popular GitOps tools that employs this pattern.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this article, I will be using Argo CD, mainly for its explicit support for the &lt;a href=&#34;#the-one-app-to-rule-them-all&#34;&gt;App of Apps pattern&lt;/a&gt; which I will cover in a section further down.&lt;/p&gt;
&lt;h2 id=&#34;argo-cd-applications&#34;&gt;Argo CD Applications&lt;/h2&gt;
&lt;p&gt;In Argo CD, a Kubernetes application are defined via an &lt;a href=&#34;https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#applications&#34;&gt;&lt;code&gt;Application&lt;/code&gt; custom resource (CR)&lt;/a&gt;. An &lt;code&gt;Application&lt;/code&gt; CR provides Argo CD with the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;source&lt;/strong&gt;: the Git repository, revision and path to where the collection of Kubernetes resource manifests are stored&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;destination&lt;/strong&gt;: the target cluster (Argo CD supports multi-cluster GitOps) and namespace to apply/deploy Kubernetes resource manifests in&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An &lt;code&gt;Application&lt;/code&gt; CR can be as simple as this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: guestbook
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/argoproj/argocd-example-apps.git
    targetRevision: HEAD
    path: guestbook
  destination:
    server: https://kubernetes.default.svc
    namespace: guestbook
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The diagram below shows how an application is deployed with Argo CD:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/argocd-basic.png&#34; alt=&#34;Basic Argo CD workflow&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;User creates/modifies and uploads Kubernetes resource manifests (e.g. Deployment, ConfigMap, Service etc) for an application into a Git repository.&lt;/li&gt;
&lt;li&gt;(&lt;strong&gt;manual step in cluster&lt;/strong&gt;) User deploys an &lt;code&gt;Application&lt;/code&gt; CR.&lt;/li&gt;
&lt;li&gt;Argo CD inspects &lt;code&gt;Application&lt;/code&gt; CR to discover the Git repository.&lt;/li&gt;
&lt;li&gt;Argo CD pulls the Kubernetes resource manifests from the Git repository.&lt;/li&gt;
&lt;li&gt;Argo CD applies/deploys the Kubernetes resource manifests into the target cluster and namespace per the &lt;code&gt;Application&lt;/code&gt; CR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And just like that, our application deployment has been turned into code. Pretty cool! But we can do better&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;the-one-app-to-rule-them-all&#34;&gt;The One App To Rule Them All&lt;/h2&gt;
&lt;p&gt;The previous workflow still involves a manual step on the cluster - creating the &lt;code&gt;Application&lt;/code&gt; CR in the cluster. If the user forgets or makes a mistake, the state of the cluster will not reflect the desired state defined in the Git repository. In order to deal with such scenarios, let&amp;rsquo;s turn to the &lt;a href=&#34;https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/#app-of-apps-pattern&#34;&gt;App of Apps pattern&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Remember that an &lt;code&gt;Application&lt;/code&gt; CR just points the agent to where the Kubernetes resource manifests are, there&amp;rsquo;s no limitation on what kind of resources it supports. Naturally, this can be extended to other &lt;code&gt;Application&lt;/code&gt; CRs! Here&amp;rsquo;s how it works:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/argocd-app-of-apps.png&#34; alt=&#34;Argo CD app of apps workflow&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with the &lt;!-- raw HTML omitted --&gt;&lt;strong&gt;red&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt; flow that shows the bootstrapping:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;User creates a bootstrap Git repository to store &lt;code&gt;Application&lt;/code&gt; CRs for the actual applications we want to deploy.&lt;/li&gt;
&lt;li&gt;User deploys a bootstrap &lt;code&gt;Application&lt;/code&gt; CR on the cluster.&lt;/li&gt;
&lt;li&gt;Argo CD inspects the bootstrap &lt;code&gt;Application&lt;/code&gt; CR to discover the bootstrap Git repository.&lt;/li&gt;
&lt;li&gt;Argo CD agent starts monitoring the Git repository for new &lt;code&gt;Application&lt;/code&gt; CRs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once the cluster has been bootstrapped (which only has to be done once when the cluster is being set up), let&amp;rsquo;s go through the &lt;!-- raw HTML omitted --&gt;&lt;strong&gt;green&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt; flow to see how applications are deployed/modified:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;User creates/modifies and uploads Kubernetes resource manifests (e.g. Deployment, ConfigMap, Service etc) for an application into an application Git repository.&lt;/li&gt;
&lt;li&gt;User creates/modifies and uploads an &lt;code&gt;Application&lt;/code&gt; CR for the above application into the bootstrap Git repository.&lt;/li&gt;
&lt;li&gt;Argo CD discovers new &lt;code&gt;Application&lt;/code&gt; CR in the bootstrap Git repository.&lt;/li&gt;
&lt;li&gt;Argo CD deploys the &lt;code&gt;Application&lt;/code&gt; CR in the cluster, and from it, discover the application Git repository.&lt;/li&gt;
&lt;li&gt;Argo CD pulls the Kubernetes resource manifests from the application Git repository&lt;/li&gt;
&lt;li&gt;Argo CD applies the Kubernetes resource manifests into the target cluster and namespace per the &lt;code&gt;Application&lt;/code&gt; CR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using the App of Apps pattern means we only have to maintain the desired state of our applications through Git, and Argo CD would automatically apply the desired state in the cluster.&lt;/p&gt;
&lt;p&gt;Phew&amp;hellip; that was a lot to take in 😅. Hopefully we are now in the right head space to move on to the next section where we automate the bootstrapping via Terraform.&lt;/p&gt;
&lt;h2 id=&#34;terraforming-gitops&#34;&gt;Terraforming GitOps&lt;/h2&gt;
&lt;p&gt;To integrate this GitOps workflow and the App of Apps pattern with the &lt;a href=&#34;#infrastructure-as-code-with-terraform&#34;&gt;IaC&lt;/a&gt; definitions, we use Terraform to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install Argo CD in the cluster&lt;/p&gt;
&lt;p&gt;I found two Terraform providers which can manage Kubernetes resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/kubernetes/latest&#34;&gt;kubernetes&lt;/a&gt; - officially supported by HashiCorp, but requires Kubernetes resources to be defined in the Terraform&amp;rsquo;s syntax HCL instead of YAML. It also does not support defining a CRD and any corresponding CRs within the same Terraform project, as the CRD won&amp;rsquo;t exist when Terraform is processing the CRs in the planning stage.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://registry.terraform.io/providers/gavinbunney/kubectl/latest&#34;&gt;kubectl&lt;/a&gt; - allows you to work in YAML and supports overwriting namespaces in the manifest. However, when performing &lt;code&gt;terraform destroy&lt;/code&gt;, it does not seem to wait for resources to be fully deleted before marking them as destroyed (see &lt;a href=&#34;https://github.com/gavinbunney/terraform-provider-kubectl/issues/109&#34;&gt;issue&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Neither of them are perfect, but I&amp;rsquo;m inclined to use the &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/kubernetes/latest&#34;&gt;kubernetes&lt;/a&gt; provider for the official support, and deviate when it doesn&amp;rsquo;t work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bootstrap the cluster with an &lt;code&gt;Application&lt;/code&gt; CR to utilize the App of Apps pattern.&lt;/p&gt;
&lt;p&gt;As explained earlier, this &lt;code&gt;Application&lt;/code&gt; CR (see &lt;a href=&#34;https://github.com/leonseng/terraform-everything/blob/master/eks-gitops/bootstrap-app.yaml.tpl&#34;&gt;manifest&lt;/a&gt;) is responsible for defining the deployment of all other applications that we actually want running in our cluster. It simply points to a repository that looks something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/leonseng/terraform-everything/tree/master/eks-gitops/demo/argocd-apps&#34;&gt;https://github.com/leonseng/terraform-everything/tree/master/eks-gitops/demo/argocd-apps&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where there are two applications to be deployed - &lt;a href=&#34;https://github.com/leonseng/terraform-everything/tree/master/eks-gitops/demo/httpbin&#34;&gt;httpbin&lt;/a&gt; and &lt;a href=&#34;https://github.com/leonseng/terraform-everything/tree/master/eks-gitops/demo/nginx&#34;&gt;nginx&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The full Terraform file can be seen here:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/leonseng/terraform-everything/blob/master/eks-gitops/argocd.tf&#34;&gt;https://github.com/leonseng/terraform-everything/blob/master/eks-gitops/argocd.tf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;finale&#34;&gt;Finale&lt;/h1&gt;
&lt;p&gt;With the Terraform files and application manifests in their respective Git repositories, we can kick off the deployment with the Terraform commands&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;terraform init
terraform apply -auto-approve
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And at the end of the day, shutting it down is as easy as running&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;terraform destroy -auto-approve
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Is it perfect? No. I need to be conscious of storing my application manifests in Git, which will slow me down. And occasionally, the destroy process fails due to some orphaned cloud resources (work in progress). But what I have now is an on/off button for my Kubernetes lab in the cloud, which should keep the IT department of my back 😁.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Automation | Learning Terraform S3 Backend</title>
      <link>/posts/learning-terraform-s3-backend/</link>
      <pubDate>Fri, 11 Jun 2021 14:52:29 +1000</pubDate>
      
      <guid>/posts/learning-terraform-s3-backend/</guid>
      <description>I have had basic experience playing with Terraform to instantiate resources in Kubernetes and AWS, but my previous attempts left me with a thought, how do I implement this at work and scale it up to the team?
Terraform creates a local state file which seems like a pain to share around a team. This is when I found out about remote backends. And this is my attempt to learn Terraform S3 backend.</description>
      <content>&lt;p&gt;I have had basic experience playing with Terraform to instantiate resources in Kubernetes and AWS, but my previous attempts left me with a thought, how do I implement this at work and scale it up to the team?&lt;/p&gt;
&lt;p&gt;Terraform creates a local state file which seems like a pain to share around a team. This is when I found out about remote backends. And this is my attempt to learn &lt;a href=&#34;https://www.terraform.io/docs/language/settings/backends/s3.html&#34;&gt;Terraform S3 backend&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;setup&#34;&gt;Setup&lt;/h1&gt;
&lt;p&gt;Terraform S3 backend stores the state file in an AWS S3 bucket, and if required, uses DynamoDB to manage state locking (highly recommended when working in a team).&lt;/p&gt;
&lt;p&gt;So&amp;hellip; to manage infrastructure using Terraform, I first need to deploy the supporting infrastructure 😅&lt;/p&gt;
&lt;p&gt;🐔 and 🥚 problem much?&lt;/p&gt;
&lt;p&gt;There are guides online to perform this setup using Terraform, but I&amp;rsquo;ll keep it simple for now and do it manually through AWS console.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create an S3 bucket&lt;/li&gt;
&lt;li&gt;Create DynamoDB table&lt;/li&gt;
&lt;li&gt;Create a role with policy to interact with S3 and DynamoDB (I called it &lt;code&gt;terraform-backend-role&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Sid&amp;quot;: &amp;quot;VisualEditor0&amp;quot;,
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;s3:PutObject&amp;quot;,
                &amp;quot;s3:GetObject&amp;quot;,
                &amp;quot;dynamodb:PutItem&amp;quot;,
                &amp;quot;dynamodb:DeleteItem&amp;quot;,
                &amp;quot;dynamodb:GetItem&amp;quot;,
                &amp;quot;s3:ListBucket&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;arn:aws:dynamodb:ap-southeast-2:&amp;lt;account ID&amp;gt;:table/TerraformLocks&amp;quot;,
                &amp;quot;arn:aws:s3:::ls-terraform-bucket/terraform.tfstate&amp;quot;,
                &amp;quot;arn:aws:s3:::ls-terraform-bucket&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Give the IAM user the appropriate policy to assume the role above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With all that done, we should be good to go.&lt;/p&gt;
&lt;h1 id=&#34;using-terraform&#34;&gt;Using Terraform&lt;/h1&gt;
&lt;p&gt;I will be using the &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/aws/latest/docs&#34;&gt;AWS provider&lt;/a&gt; to create the EC2 instance.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# main.tf
terraform {
  backend &amp;quot;s3&amp;quot; {
    bucket = &amp;quot;ls-terraform-bucket&amp;quot;
    key = &amp;quot;terraform.tfstate&amp;quot;
    region = &amp;quot;ap-southeast-2&amp;quot;
    encrypt = true

    dynamodb_table = &amp;quot;TerraformLocks&amp;quot;

    role_arn = &amp;quot;arn:aws:iam::&amp;lt;account_id&amp;gt;:role/terraform-backend-role&amp;quot;
  }
}

provider &amp;quot;aws&amp;quot; {
  region = &amp;quot;ap-southeast-2&amp;quot;
}

resource &amp;quot;aws_instance&amp;quot; &amp;quot;terraform-test&amp;quot; {
  ami = &amp;quot;ami-0186908e2fdeea8f3&amp;quot;
  instance_type = &amp;quot;t3.micro&amp;quot;
  availability_zone = &amp;quot;ap-southeast-2a&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To run Terraform apply, I need to provide it with my AWS authentication details, which I do by exporting the &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; environment variables.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ export AWS_ACCESS_KEY_ID=&amp;quot;anaccesskey&amp;quot;
$ export AWS_SECRET_ACCESS_KEY=&amp;quot;asecretkey&amp;quot;
$ terraform plan
$ terraform apply
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I quickly ran into permission errors, but the error message isn&amp;rsquo;t great.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;╷
│ Error: Error launching source instance: UnauthorizedOperation: You are not authorized to perform this operation. Encoded authorization failure message: &amp;lt;encoded error message&amp;gt;
│       status code: 403, request id: cc586b96-11b7-4a68-a54c-24bf4bce5419
│
│   with aws_instance.terraform-test,
│   on main.tf line 21, in resource &amp;quot;aws_instance&amp;quot; &amp;quot;terraform-test&amp;quot;:
│   21: resource &amp;quot;aws_instance&amp;quot; &amp;quot;terraform-test&amp;quot; {
│
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Enabling debug logs when running &lt;code&gt;terraform apply&lt;/code&gt; didn&amp;rsquo;t reveal anything useful for this particular error either&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ TF_LOG=DEBUG terraform apply
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A quick Google shows that I can use AWS CLI to &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html&#34;&gt;decode the error message&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ aws sts decode-authorization-message --encoded-message &amp;lt;encoded error message&amp;gt; --query DecodedMessage --output text | jq .
{
  &amp;quot;allowed&amp;quot;: false,
  ...
  &amp;quot;context&amp;quot;: {
    &amp;quot;action&amp;quot;: &amp;quot;ec2:RunInstances&amp;quot;,
    &amp;quot;resource&amp;quot;: &amp;quot;arn:aws:ec2:ap-southeast-2:&amp;lt;account ID&amp;gt;:instance/*&amp;quot;,
    ...
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;which tells me that I don&amp;rsquo;t have permission to create an EC2 instance. Of course! Permission to actually perform what I want to do on AWS! I created the necessary role and policy to interact with EC2 (I called it &lt;code&gt;test-project&lt;/code&gt;, and gave my IAM user permission to assume the role. Once that&amp;rsquo;s been done, I added the &lt;code&gt;assume_role&lt;/code&gt; attribute to the AWS provider&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# main.tf
terraform {
  backend &amp;quot;s3&amp;quot; {
    bucket = &amp;quot;ls-terraform-bucket&amp;quot;
    key = &amp;quot;terraform.tfstate&amp;quot;
    region = &amp;quot;ap-southeast-2&amp;quot;
    encrypt = true

    dynamodb_table = &amp;quot;TerraformLocks&amp;quot;

    role_arn = &amp;quot;arn:aws:iam::&amp;lt;account_id&amp;gt;:role/terraform-backend-role&amp;quot;
  }
}

provider &amp;quot;aws&amp;quot; {
  assume_role {
    role_arn = &amp;quot;arn:aws:iam::&amp;lt;account_id&amp;gt;:role/test-project&amp;quot;
  }
  region = &amp;quot;ap-southeast-2&amp;quot;
}

resource &amp;quot;aws_instance&amp;quot; &amp;quot;terraform-test&amp;quot; {
  ami = &amp;quot;ami-0186908e2fdeea8f3&amp;quot;
  instance_type = &amp;quot;t3.micro&amp;quot;
  availability_zone = &amp;quot;ap-southeast-2a&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With that, &lt;code&gt;terraform apply&lt;/code&gt; was able to run successfully and created the EC2 instance&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ terraform apply

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # aws_instance.terraform-test will be created
  + resource &amp;quot;aws_instance&amp;quot; &amp;quot;terraform-test&amp;quot; {
      + ami                                  = &amp;quot;ami-0186908e2fdeea8f3&amp;quot;
      + arn                                  = (known after apply)
      + associate_public_ip_address          = (known after apply)
      + availability_zone                    = &amp;quot;ap-southeast-2a&amp;quot;
      + cpu_core_count                       = (known after apply)
      + cpu_threads_per_core                 = (known after apply)
      + get_password_data                    = false
      + host_id                              = (known after apply)
      + id                                   = (known after apply)
      + instance_initiated_shutdown_behavior = (known after apply)
      + instance_state                       = (known after apply)
      + instance_type                        = &amp;quot;t3.micro&amp;quot;
      + ipv6_address_count                   = (known after apply)
      + ipv6_addresses                       = (known after apply)
      + key_name                             = (known after apply)
      + outpost_arn                          = (known after apply)
      + password_data                        = (known after apply)
      + placement_group                      = (known after apply)
      + primary_network_interface_id         = (known after apply)
      + private_dns                          = (known after apply)
      + private_ip                           = (known after apply)
      + public_dns                           = (known after apply)
      + public_ip                            = (known after apply)
      + secondary_private_ips                = (known after apply)
      + security_groups                      = (known after apply)
      + source_dest_check                    = true
      + subnet_id                            = (known after apply)
      + tags                                 = {
          + &amp;quot;Name&amp;quot; = &amp;quot;Terraform EC2&amp;quot;
        }
      + tags_all                             = {
          + &amp;quot;Name&amp;quot; = &amp;quot;Terraform EC2&amp;quot;
        }
      + tenancy                              = (known after apply)
      + vpc_security_group_ids               = (known after apply)

      + capacity_reservation_specification {
          + capacity_reservation_preference = (known after apply)

          + capacity_reservation_target {
              + capacity_reservation_id = (known after apply)
            }
        }

      + ebs_block_device {
          + delete_on_termination = (known after apply)
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = (known after apply)
          + kms_key_id            = (known after apply)
          + snapshot_id           = (known after apply)
          + tags                  = (known after apply)
          + throughput            = (known after apply)
          + volume_id             = (known after apply)
          + volume_size           = (known after apply)
          + volume_type           = (known after apply)
        }

      + enclave_options {
          + enabled = (known after apply)
        }

      + ephemeral_block_device {
          + device_name  = (known after apply)
          + no_device    = (known after apply)
          + virtual_name = (known after apply)
        }

      + metadata_options {
          + http_endpoint               = (known after apply)
          + http_put_response_hop_limit = (known after apply)
          + http_tokens                 = (known after apply)
        }

      + network_interface {
          + delete_on_termination = (known after apply)
          + device_index          = (known after apply)
          + network_interface_id  = (known after apply)
        }

      + root_block_device {
          + delete_on_termination = (known after apply)
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = (known after apply)
          + kms_key_id            = (known after apply)
          + tags                  = (known after apply)
          + throughput            = (known after apply)
          + volume_id             = (known after apply)
          + volume_size           = (known after apply)
          + volume_type           = (known after apply)
        }
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only &#39;yes&#39; will be accepted to approve.

  Enter a value: yes

aws_instance.terraform-test: Creating...
aws_instance.terraform-test: Still creating... [10s elapsed]
aws_instance.terraform-test: Creation complete after 13s [id=i-05e44bf5cfbb98977]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;With that, I am now able to use Terraform with the state tracked remotely. Along with state locking, multiple people in the team can access the same state file safely, as long as their IAM users have the correct permission to assume the role to read/write to S3 and DynamoDB.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
